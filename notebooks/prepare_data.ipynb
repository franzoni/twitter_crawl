{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musella/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../python\")\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1600\n",
    "glove_dim = 25\n",
    "hash_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embed = utils.load_glove_embedding(utils.glove_embedding_path(25))\n",
    "hash_embed = utils.load_glove_embedding('../data/models/hastags/hash_vectors.d'+str(hash_dim)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"../data/preproc/tweets.hd5\")\n",
    "\n",
    "tk = Tokenizer(num_words=max_words,\n",
    "               filters=\"\", # already applied\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(df[\"preproc_text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "words_embed_mtx, words_unk = utils.fill_embedding_matrix(tk.word_index,words_embed,max_words,glove_dim)\n",
    "hash_embed_mtx, hash_unk = utils.fill_embedding_matrix(tk.word_index,hash_embed,max_words,hash_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<stop>': 2,\n",
       " '<trunc>': 5,\n",
       " 'chatbot': 961,\n",
       " 'chatbots': 738,\n",
       " 'convolutional': 689,\n",
       " 'datasciencectrl': 1099,\n",
       " 'datax': 1518,\n",
       " 'daysofmlcode': 1334,\n",
       " 'deeplearn007': 1563,\n",
       " 'interpretability': 1112,\n",
       " 'ipfconline1': 1414,\n",
       " 'kaggle': 1269,\n",
       " 'kdnuggets': 1157,\n",
       " 'kirkdborne': 735,\n",
       " 'machinelearning': 491,\n",
       " 'mikequindazzi': 183,\n",
       " 'neurips': 424,\n",
       " 'neurips2018': 1560,\n",
       " 'nodexl': 994,\n",
       " 'poptimize': 767,\n",
       " 'pulipaka': 848,\n",
       " 'pytorch': 1267,\n",
       " 'spirosmargaris': 1523,\n",
       " 'tensorflow': 298,\n",
       " 'v1': 1263,\n",
       " 'vanloon': 104,\n",
       " 'variational': 1439,\n",
       " 'verisk': 1593,\n",
       " 'whova': 566}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = { word: tk.word_index[word] for word in set(words_unk).intersection(set(hash_unk)) }\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4017a49bb19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstop_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<stop>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstop_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "stop_mtx = np.zeros( (max_words+1, 3) )\n",
    "\n",
    "stop_mtx[unk.pop(\"<trunc>\"),-2] = 1.\n",
    "stop_mtx[unk.pop(\"<stop>\"),-1] = 1.\n",
    "\n",
    "stop_mtx[unk.values(),-3] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_idx = list(unk.values())\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(words_embed_mtx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(hash_embed_mtx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir ../data/models/sequences\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "with open(\"../data/models/sequences/info.json\",\"w+\") as out:\n",
    "    save = dict(max_words=max_words,unk_idx=unk_idx)\n",
    "    out.write(json.dumps(save))\n",
    "    \n",
    "with open('../data/models/sequences/tokenizer.pkl','wb+') as out:\n",
    "    out.write( pickle.dumps(tk) )\n",
    "    \n",
    "np.save('../data/models/sequences/embed_mtx.npy',words_embed_mtx)\n",
    "np.save('../data/models/sequences/hash_mtx.npy',hash_embed_mtx)\n",
    "np.save('../data/models/sequences/hash_mtx.npy',stop_mtx)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df[\"sequences\"] = tk.texts_to_sequences(df[\"preproc_text\"])\n",
    "\n",
    "df.to_hdf(\"../data/preproc/sequences.hd5\",\"squences\",columns=[\"sequences\"],mode=\"w\")\n",
    "\n",
    "# index = np.arange(df.index.shape[0]).astype(np.int)\n",
    "# train_idx, test_idx = train_test_split(index,test_size=0.2,random_state=123456)\n",
    "# df_train = df.iloc[train_idx]\n",
    "# df_test = df.iloc[test_idx]\n",
    "# df_train.to_hdf(\"../data/preproc/sequences.hd5\",\"train\",columns=[\"sequences\"])\n",
    "# df_test.to_hdf(\"../data/preproc/sequences.hd5\",\"test\",columns=[\"sequences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tk.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
