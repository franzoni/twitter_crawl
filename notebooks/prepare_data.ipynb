{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musella/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../python\")\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1600\n",
    "glove_dim = 25\n",
    "hash_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embed = utils.load_glove_embedding(utils.glove_embedding_path(25))\n",
    "hash_embed = utils.load_glove_embedding('../data/models/hastags/hash_vectors.d'+str(hash_dim)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"../data/preproc/tweets.hd5\")\n",
    "\n",
    "tk = Tokenizer(num_words=max_words,\n",
    "               filters=\"\", # already applied\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(df[\"preproc_text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "words_embed_mtx, words_unk = utils.fill_embedding_matrix(tk.word_index,words_embed,max_words,glove_dim)\n",
    "hash_embed_mtx, hash_unk = utils.fill_embedding_matrix(tk.word_index,hash_embed,max_words,hash_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#mercerchats': 1233,\n",
       " '#opensource': 1512,\n",
       " '#payments': 1398,\n",
       " '<stop>': 3,\n",
       " '<trunc>': 4,\n",
       " 'agritech': 1322,\n",
       " 'bottou': 1339,\n",
       " 'chatbot': 888,\n",
       " 'chatbots': 1097,\n",
       " 'convolutional': 623,\n",
       " 'datax': 700,\n",
       " 'daysofmlcode': 1227,\n",
       " 'deeplearn007': 254,\n",
       " 'interpretability': 1543,\n",
       " 'ipfconline1': 970,\n",
       " 'jimmarous': 620,\n",
       " 'kirkdborne': 1284,\n",
       " 'machinelearning': 1041,\n",
       " 'mikequindazzi': 265,\n",
       " 'neurips': 325,\n",
       " 'nodexl': 583,\n",
       " 'parkinson’s': 1329,\n",
       " 'pineau': 1065,\n",
       " 'preprint': 1527,\n",
       " 'prioirites': 1171,\n",
       " 'probabilistic': 1013,\n",
       " 'pulipaka': 1582,\n",
       " 'reproducibility': 1239,\n",
       " 'selectscience': 1438,\n",
       " 'spirosmargaris': 575,\n",
       " 'tencent’s': 1357,\n",
       " 'tensorflow': 492,\n",
       " 'vanloon': 174,\n",
       " 'variational': 1020,\n",
       " 'verisk': 1285,\n",
       " 'whova': 1133,\n",
       " 'world’s': 1274,\n",
       " 'yape': 902}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = { word: tk.word_index[word] for word in set(words_unk).intersection(set(hash_unk)) }\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_mtx = np.zeros( (max_words+1, 2) )\n",
    "\n",
    "stop_mtx[unk.pop(\"<trunc>\"),0] = 1.\n",
    "stop_mtx[unk.pop(\"<stop>\"),1] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#mercerchats': 1233,\n",
       " '#opensource': 1512,\n",
       " '#payments': 1398,\n",
       " 'agritech': 1322,\n",
       " 'bottou': 1339,\n",
       " 'chatbot': 888,\n",
       " 'chatbots': 1097,\n",
       " 'convolutional': 623,\n",
       " 'datax': 700,\n",
       " 'daysofmlcode': 1227,\n",
       " 'deeplearn007': 254,\n",
       " 'interpretability': 1543,\n",
       " 'ipfconline1': 970,\n",
       " 'jimmarous': 620,\n",
       " 'kirkdborne': 1284,\n",
       " 'machinelearning': 1041,\n",
       " 'mikequindazzi': 265,\n",
       " 'neurips': 325,\n",
       " 'nodexl': 583,\n",
       " 'parkinson’s': 1329,\n",
       " 'pineau': 1065,\n",
       " 'preprint': 1527,\n",
       " 'prioirites': 1171,\n",
       " 'probabilistic': 1013,\n",
       " 'pulipaka': 1582,\n",
       " 'reproducibility': 1239,\n",
       " 'selectscience': 1438,\n",
       " 'spirosmargaris': 575,\n",
       " 'tencent’s': 1357,\n",
       " 'tensorflow': 492,\n",
       " 'vanloon': 174,\n",
       " 'variational': 1020,\n",
       " 'verisk': 1285,\n",
       " 'whova': 1133,\n",
       " 'world’s': 1274,\n",
       " 'yape': 902}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_idx = list(unk.values())\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(words_embed_mtx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(hash_embed_mtx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir ../data/models/sequences\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "with open(\"../data/models/sequences/info.json\",\"w+\") as out:\n",
    "    save = dict(max_words=max_words,unk_idx=unk_idx)\n",
    "    out.write(json.dumps(save))\n",
    "    \n",
    "with open('../data/models/sequences/tokenizer.pkl','wb+') as out:\n",
    "    out.write( pickle.dumps(tk) )\n",
    "    \n",
    "np.save('../data/models/sequences/embed_mtx.npy',words_embed_mtx)\n",
    "np.save('../data/models/sequences/hash_mtx.npy',hash_embed_mtx)\n",
    "np.save('../data/models/sequences/hash_mtx.npy',stop_mtx)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musella/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:1471: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['lang', 'text', 'user/name', 'preproc_text', 'sequences']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df[\"sequences\"] = tk.texts_to_sequences(df[\"preproc_text\"])\n",
    "\n",
    "df.to_hdf(\"../data/preproc/sequences.hd5\",\"squences\",columns=[\"sequences\"],mode=\"w\")\n",
    "\n",
    "# index = np.arange(df.index.shape[0]).astype(np.int)\n",
    "# train_idx, test_idx = train_test_split(index,test_size=0.2,random_state=123456)\n",
    "# df_train = df.iloc[train_idx]\n",
    "# df_test = df.iloc[test_idx]\n",
    "# df_train.to_hdf(\"../data/preproc/sequences.hd5\",\"train\",columns=[\"sequences\"])\n",
    "# df_test.to_hdf(\"../data/preproc/sequences.hd5\",\"test\",columns=[\"sequences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tk.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
